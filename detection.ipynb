{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab367d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] starting video stream...\n",
      "(1, 1, 200, 7)\n",
      "[8.0188233e-01 1.7010699e-01 4.8094927e-05 1.1178215e-04 2.7850917e-02]\n",
      "angry\n",
      "(1, 1, 200, 7)\n",
      "[5.9224904e-01 1.9516951e-01 2.4206334e-05 8.7724526e-05 2.1246949e-01]\n",
      "angry\n",
      "(1, 1, 200, 7)\n",
      "[6.8895805e-01 1.5384290e-01 1.3356798e-05 7.6480945e-05 1.5710916e-01]\n",
      "angry\n",
      "(1, 1, 200, 7)\n",
      "[6.8382943e-01 8.4759705e-02 6.5462400e-06 8.0127589e-05 2.3132427e-01]\n",
      "angry\n",
      "(1, 1, 200, 7)\n",
      "[3.1902738e-02 6.4510530e-01 6.0449471e-05 7.2210474e-05 3.2285935e-01]\n",
      "happy\n",
      "(1, 1, 200, 7)\n",
      "[9.4612181e-02 4.9828216e-01 5.4297016e-05 1.0151315e-04 4.0694985e-01]\n",
      "happy\n",
      "(1, 1, 200, 7)\n",
      "[4.0073189e-01 1.0840924e-01 1.5756477e-05 9.3711766e-05 4.9074939e-01]\n",
      "surprise\n",
      "(1, 1, 200, 7)\n",
      "[2.7933159e-01 1.5396679e-02 1.5529567e-06 5.1816023e-05 7.0521832e-01]\n",
      "surprise\n",
      "(1, 1, 200, 7)\n",
      "[7.4937081e-01 4.1542074e-04 9.7838438e-08 3.9284088e-05 2.5017446e-01]\n",
      "angry\n",
      "(1, 1, 200, 7)\n",
      "[6.1208200e-01 8.6496334e-04 1.7683202e-07 5.6576810e-05 3.8699627e-01]\n",
      "angry\n",
      "(1, 1, 200, 7)\n",
      "[6.4754802e-01 9.1091887e-04 1.7794852e-07 5.1470204e-05 3.5148937e-01]\n",
      "angry\n",
      "(1, 1, 200, 7)\n",
      "[4.1138270e-01 6.1720414e-03 6.5832461e-07 5.2223684e-05 5.8239239e-01]\n",
      "surprise\n",
      "(1, 1, 200, 7)\n",
      "[4.0616220e-01 5.1572252e-02 9.8935643e-06 1.3067685e-04 5.4212499e-01]\n",
      "surprise\n",
      "(1, 1, 200, 7)\n",
      "[4.57310170e-01 4.08049999e-03 1.11858253e-06 1.05272404e-04\n",
      " 5.38502991e-01]\n",
      "surprise\n",
      "(1, 1, 200, 7)\n",
      "[8.1790902e-02 7.5860694e-02 1.4852971e-05 7.9777892e-05 8.4225374e-01]\n",
      "surprise\n",
      "(1, 1, 200, 7)\n",
      "[3.1188071e-01 6.8043679e-01 8.3486875e-06 2.8961007e-05 7.6451628e-03]\n",
      "happy\n",
      "(1, 1, 200, 7)\n",
      "[7.9134768e-01 1.9079684e-01 5.5812332e-05 1.8027563e-04 1.7619422e-02]\n",
      "angry\n",
      "(1, 1, 200, 7)\n",
      "[9.70832586e-01 1.52527224e-02 1.64089579e-05 9.67596425e-05\n",
      " 1.38015635e-02]\n",
      "angry\n",
      "(1, 1, 200, 7)\n",
      "[9.8698282e-01 5.7333340e-03 6.1588667e-06 5.9145677e-05 7.2185034e-03]\n",
      "angry\n",
      "(1, 1, 200, 7)\n",
      "[9.6112394e-01 2.0323817e-02 1.8014311e-05 1.1082309e-04 1.8423447e-02]\n",
      "angry\n",
      "(1, 1, 200, 7)\n",
      "[9.7023207e-01 2.1873275e-02 1.5918087e-05 7.7928591e-05 7.8007001e-03]\n",
      "angry\n",
      "(1, 1, 200, 7)\n",
      "[9.4928485e-01 2.2961983e-02 8.6886048e-06 7.0918009e-05 2.7673636e-02]\n",
      "angry\n",
      "(1, 1, 200, 7)\n",
      "[9.1947454e-01 4.5292404e-02 1.5500927e-05 8.2069841e-05 3.5135604e-02]\n",
      "angry\n",
      "(1, 1, 200, 7)\n",
      "[2.1202543e-01 7.6722193e-01 3.8375161e-05 8.2239174e-05 2.0632092e-02]\n",
      "happy\n",
      "(1, 1, 200, 7)\n",
      "[3.404158e-01 5.717087e-01 8.009291e-05 1.763457e-04 8.761903e-02]\n",
      "happy\n",
      "(1, 1, 200, 7)\n",
      "[2.8045341e-02 2.0327482e-01 8.0780892e-06 2.6696889e-05 7.6864505e-01]\n",
      "surprise\n",
      "(1, 1, 200, 7)\n",
      "[2.4151799e-01 4.7497702e-01 1.7376115e-05 5.1356423e-05 2.8343624e-01]\n",
      "happy\n",
      "(1, 1, 200, 7)\n",
      "[5.0090086e-01 3.9261248e-02 4.6247642e-06 7.0076530e-05 4.5976317e-01]\n",
      "angry\n",
      "(1, 1, 200, 7)\n",
      "[9.1401786e-01 9.5068421e-03 1.5245539e-06 2.9117469e-05 7.6444685e-02]\n",
      "angry\n",
      "(1, 1, 200, 7)\n",
      "[8.1338108e-01 1.3233733e-03 5.2769599e-07 3.4429551e-05 1.8526065e-01]\n",
      "angry\n",
      "(1, 1, 200, 7)\n",
      "[5.6036718e-02 8.5175240e-01 7.0312504e-05 7.8167970e-05 9.2062406e-02]\n",
      "happy\n",
      "(1, 1, 200, 7)\n",
      "[2.2769473e-02 8.0755055e-01 1.2948278e-05 2.0616302e-05 1.6964647e-01]\n",
      "happy\n",
      "(1, 1, 200, 7)\n",
      "[7.7577546e-02 3.6849773e-01 2.6161804e-05 6.0466478e-05 5.5383807e-01]\n",
      "surprise\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import load_model\n",
    "from imutils.video import VideoStream\n",
    "import numpy as np\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "def detect_and_predict_emot(frame, faceNet, maskNet):\n",
    "\t# grab the dimensions of the frame and then construct a blob\n",
    "\t# from it\n",
    "    (h, w) = frame.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1.0, (224, 224), (104.0, 177.0, 123.0)) #preprocessing :- image, scaling,size,mean(RGB)  three tupple\n",
    "\n",
    "\n",
    "    # pass the blob through the network and obtain the face detections\n",
    "    faceNet.setInput(blob)\n",
    "    detections = faceNet.forward()\n",
    "    print(detections.shape)\n",
    "\n",
    "    # initialize our list of faces, their corresponding locations,\n",
    "    # and the list of predictions from our face mask network\n",
    "    faces = [] #list of face\n",
    "    locs = []\n",
    "    preds = []\n",
    "\n",
    "    # loop over the detections\n",
    "    for i in range(0, detections.shape[2]):\n",
    "        # extract the confidence (i.e., probability) associated with\n",
    "        # the detection\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "\n",
    "        # filter out weak detections by ensuring the confidence is\n",
    "        # greater than the minimum confidence\n",
    "        if confidence > 0.5:\n",
    "            # compute the (x, y)-coordinates of the bounding box for\n",
    "            # the object\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "            # ensure the bounding boxes fall within the dimensions of\n",
    "            # the frame\n",
    "            (startX, startY) = (max(0, startX), max(0, startY))\n",
    "            (endX, endY) = (min(w - 1, endX), min(h - 1, endY))\n",
    "\n",
    "            # extract the face ROI, convert it from BGR to RGB channel\n",
    "            # ordering, resize it to 224x224, and preprocess it\n",
    "            face = frame[startY:endY, startX:endX]\n",
    "            face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB) \n",
    "            face = cv2.resize(face, (224, 224))\n",
    "            face = img_to_array(face)\n",
    "            face = preprocess_input(face)\n",
    "\n",
    "            # add the face and bounding boxes to their respective\n",
    "            # lists\n",
    "            faces.append(face)  #more then one face \n",
    "            locs.append((startX, startY, endX, endY))\n",
    "\n",
    "    # only make a predictions if at least one face was detected\n",
    "    if len(faces) > 0:\n",
    "        # for faster inference we'll make batch predictions on *all*\n",
    "        # faces at the same time rather than one-by-one predictions\n",
    "        # in the above `for` loop\n",
    "        faces = np.array(faces, dtype=\"float32\")\n",
    "        preds = emotNet.predict(faces, batch_size=32)\n",
    "\n",
    "    # return a 2-tuple of the face locations and their corresponding\n",
    "    # locations\n",
    "    return (locs, preds)\n",
    "\n",
    "# load our serialized face detector model from disk\n",
    "prototxtPath = r\"face_detector\\deploy.prototxt\"\n",
    "weightsPath = r\"face_detector\\res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "faceNet = cv2.dnn.readNet(prototxtPath, weightsPath)\n",
    "\n",
    "# load the face mask detector model from disk\n",
    "# emotNet = load_model(\"emotion_detector.model\")\n",
    "emotNet = load_model(\"model.h5\")\n",
    "# initialize the video stream\n",
    "print(\"[INFO] starting video stream...\")\n",
    "vs = VideoStream(src=0).start()\n",
    "classes=['angry','happy','neutral','sad','surprise']\n",
    "# loop over the frames from the video stream\n",
    "while True:\n",
    "    # grab the frame from the threaded video stream and resize it\n",
    "    # to have a maximum width of 400 pixels\n",
    "    frame = vs.read()\n",
    "    frame = imutils.resize(frame, width=400)\n",
    "\n",
    "    # detect faces in the frame and determine if they are wearing a\n",
    "    # face mask or not\n",
    "    (locs, preds) = detect_and_predict_emot(frame, faceNet, emotNet)\n",
    "\n",
    "    # loop over the detected face locations and their corresponding\n",
    "    # locations\n",
    "    for (box, pred) in zip(locs, preds):\n",
    "        # unpack the bounding box and predictions\n",
    "        (startX, startY, endX, endY) = box #locs has 4 variables\n",
    "#         (angry,happy,neutral,sad,surprise) = pred # [probab, probab]\n",
    "        predicted_class=classes[np.argmax(pred)]\n",
    "        print(pred)\n",
    "        print(predicted_class)\n",
    "\n",
    "\n",
    "        # determine the class label and color we'll use to draw\n",
    "        # the bounding box and text\n",
    "#         label = \"Mask\" if mask > withoutMask else \"No Mask\"\n",
    "        label= predicted_class\n",
    "        color = (0, 255, 0) \n",
    "\n",
    "        # include the probability in the label\n",
    "        label = \"{}: {:.2f}%\".format(label, np.max(pred) * 100)\n",
    "\n",
    "        # display the label and bounding box rectangle on the output\n",
    "        # frame\n",
    "        cv2.putText(frame, label, (startX, startY - 10),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
    "        cv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n",
    "\n",
    "    # show the output frame\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    # if the `q` key was pressed, break from the loop\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "\n",
    "# do a bit of cleanup\n",
    "cv2.destroyAllWindows()\n",
    "vs.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "258d0012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95492321\n"
     ]
    }
   ],
   "source": [
    "l=[1.7594839e-03, 9.5492321e-01 ,1.2837408e-05 ,1.0396684e-05, 4.3294121e-02]\n",
    "o=np.max(l)\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d21b969",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
